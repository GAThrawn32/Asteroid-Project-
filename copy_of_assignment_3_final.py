# -*- coding: utf-8 -*-
"""Copy of Assignment 3: Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FwPcvtNIuMeY7eOxFnM97Do0TPy1Rt3z

# Introduction

Our team is attempting to develop a neural network that can help astronomers and other scientists predict whether an incoming asteroid is hazardous or not. NASA routinely monitors for Potentially Hazardous Asteroids, or PHAs, as part of their core duties. These asteroids are objects that are larger than a given size and pass closer than a given distance to earth. Detection of such asteroids poses a challenge for astronomers, as new objects are frequently discovered for the first time and measured data from these objects can include dozens of variables, making the relationships difficult to uncover.

We believe a neural network is well-suited to solve such a problem. Neural networks are ideal for uncovering relationships between multiple features and are also particularly good at classification problems as a result of uncovering these relationships. This model would receive inputs of dozens of features including speeds, sizes, and orbit information, and output a prediction of whether an object is likely hazardous to earth or not. In doing so, we hope the model will not only help the predictions improve, but also help researchers uncover which features of new objects are more likely to indicate that it could pose a hazard to earth.

# EDA

Our underlying dataset comes from NASA and we accessed it through the Kaggle repository of datasets. The data includes over 35 features of different identified asteroids, and also labels each asteroid as either hazardous or non-hazardous. We've identified this binary variable as our target variable.
"""

import pandas as pd
import io

# Import the dataset nasa.csv
from google.colab import files
uploaded = files.upload()

# Load the CSV file into a DataFrame
df = pd.read_csv(io.BytesIO(uploaded['nasa.csv']))

"""We'll look at the first five rows of the data to better understand the dataset:"""

df.head()

df.info()

"""Since so many of the features are labeled as technical astronomical terms, we've generated a short list to explain each variable:

- **Neo Reference ID**: A unique identifier for the Near Earth Object (NEO) in NASA's database.
- **Name**: Another identifier for the NEO, which could be a number or name.
- **Absolute Magnitude**: A measure of the NEO's brightness as seen from a standard distance. This value is indicative of the size of the object; lower values indicate larger NEOs.
- **Est Dia in KM(min/max)**: Estimated diameter of the NEO in kilometers, provided as a range from minimum to maximum size.
- **Est Dia in M(min/max)**: Estimated diameter of the NEO in meters, provided as a range from minimum to maximum size.
- **Est Dia in Miles(min/max)**: Estimated diameter of the NEO in miles, provided as a range from minimum to maximum size.
- **Est Dia in Feet(min/max)**: Estimated diameter of the NEO in feet, provided as a range from minimum to maximum size.



- **Orbit ID, Orbit Determination Date, Orbit Uncertainty**: Details regarding the NEO's orbit, including an identification number, the date the orbit was determined, and the uncertainty in the orbit determination.
- **Minimum Orbit Intersection, Jupiter Tisserand Invariant, Epoch Osculation, Eccentricity**: These columns provide more specifics about the NEO's orbit, such as its minimum distance from Earth's orbit, its orbital relationship with Jupiter, and its shape.
- **Semi Major Axis, Inclination, Asc Node Longitude, Orbital Period**: Further orbital parameters, including the half-major axis of the orbit, the tilt of the orbit relative to the solar system's plane, the longitude of the ascending node, and the time it takes to complete one orbit around the Sun.
- **Perihelion Distance, Perihelion Arg, Aphelion Dist, Perihelion Time**: Details about the closest and farthest points in the NEO's orbit relative to the Sun, and the timing of the perihelion passage.
- **Mean Anomaly, Mean Motion, Equinox**: Parameters describing the NEO's current position in its orbit, its average speed, and the equinox used as a reference for the orbital elements.

- **Hazardous**: A boolean value indicating whether the NEO is potentially hazardous to Earth.

As briefly mentioned before, our target variable will be the "hazardous" feature, since our problem requires classifying asteroids as either hazardous or non-hazardous. Our data is already labeled with each asteroid bearing a true or false boolean value about if it is hazardous to earth.

Most of the features are numeric, but a few are objects. Depending on how many categories these variables have, we may drop some entirely. Additionally, some of the features are repetitive, such as listing multiple measures of speed or size in different units. We plan to drop or combine some of these metrics to simplify the model and create averages. We'll also check for missing values and normalize any of the numeric data for better model training.

To begin cleaning the data, we want to check for missing values.
"""

missing_values = df.isnull().sum()

missing_values

"""As you can see, we do not have any missing values in our data.

Next we want to normalize the data, to aid the training process. While this does decrease interpretability, it will make our model more accurate.
"""

numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
numerical_columns.remove('Neo Reference ID')
numerical_columns.remove('Name')

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

df.head()

df['Close Approach Date'] = pd.to_datetime(df['Close Approach Date'])
df['Orbit Determination Date'] = pd.to_datetime(df['Orbit Determination Date'])

"""Next, we will visualize our date to better examine each variable, which will be helpful when performing feature selection."""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns

# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
from matplotlib.colors import ListedColormap


for column in df:
        plt.figure(figsize=(17,1))
        sns.boxplot(data=df, x=column)

"""Additionally, we used some other diagrams to visualize data such as the absolute magnitude, the close approaches over time, and the number of hazardous vs non-hazardous asteroids in the data set."""

ig, ax = plt.subplots(2, 2, figsize=(14, 10))


ax[0, 0].hist(df['Absolute Magnitude'], bins=30, color='blue', alpha=0.7)
ax[0, 0].set_title('Histogram of Absolute Magnitude')
ax[0, 0].set_xlabel('Absolute Magnitude')
ax[0, 0].set_ylabel('Frequency')


ax[0, 1].scatter(df['Absolute Magnitude'], df['Miss Dist.(Astronomical)'], color='red', alpha=0.5)
ax[0, 1].set_title('Absolute Magnitude vs. Miss Distance (Astronomical)')
ax[0, 1].set_xlabel('Absolute Magnitude')
ax[0, 1].set_ylabel('Miss Distance (Astronomical)')


df['Year'] = df['Close Approach Date'].dt.year
yearly_counts = df['Year'].value_counts().sort_index()
ax[1, 0].plot(yearly_counts.index, yearly_counts.values, marker='o', linestyle='-', color='green')
ax[1, 0].set_title('Close Approaches Over Time')
ax[1, 0].set_xlabel('Year')
ax[1, 0].set_ylabel('Number of Close Approaches')


hazardous_counts = df['Hazardous'].value_counts()
ax[1, 1].pie(hazardous_counts, labels=hazardous_counts.index, autopct='%1.1f%%', colors=['lightcoral', 'lightblue'])
ax[1, 1].set_title('Proportion of Hazardous vs. Non-Hazardous NEOs')

plt.tight_layout()
plt.show()

"""Some variables stand out as those that could be dropped: orbiting body, due to Earth being the only object; the Equinox, as it is the same value for every NEO; and the Orbit Determination Date, as this value does not say anything about the asteroid and instead reflects on human capabilities."""

df = df.drop(columns=['Orbiting Body', 'Equinox', 'Orbit Determination Date'])


df = df.drop(columns=['Close Approach Date','Epoch Date Close Approach', 'Perihelion Time'])

"""Next, we need to address that our target, whether the asteroid is hazardous or not, is an object and not a float. We can use one-hot encoding to convert the object into a float."""

from sklearn.preprocessing import OneHotEncoder


df["Hazardous"].value_counts()


df_encoded = df.copy()


encoder = OneHotEncoder(sparse=False)
hazardous_encoded = encoder.fit_transform(df_encoded[['Hazardous']])


encoded_df = pd.DataFrame(hazardous_encoded, columns=['Hazardous_False', 'Hazardous_True'])


df_encoded = pd.concat([df, encoded_df], axis=1)


df_encoded.drop('Hazardous', axis=1, inplace=True)


df_encoded.drop('Hazardous_False', axis=1, inplace=True)

df_encoded.info()

"""Next, we will contine to adjust features. First we will combine two features, the estimated diameter maximum and miminmum to create an average estimated diameter measurment. We will also continue to remove either irrelevant features or features that are duplicative, such as the diameter estimates in different measurments."""

df_encoded['Est Dia in KM(avg)'] = (df['Est Dia in KM(min)'] + df['Est Dia in KM(max)']) / 2

df_cleaned = df_encoded.drop(['Neo Reference ID', 'Year','Name', 'Orbit ID', 'Est Dia in KM(min)', 'Est Dia in KM(max)', 'Est Dia in M(min)', 'Est Dia in M(max)', 'Est Dia in Miles(min)', 'Est Dia in Miles(max)', 'Est Dia in Feet(min)', 'Est Dia in Feet(max)'], axis=1)

df_cleaned.head()

df_cleaned.info()

"""# **Model Fitting**

Now that the data is cleaned, we'll tackle building our neural network. For building this model, we chose to use the ReLu activation function for three initial layers and the sigmoid activation function for the output layer. Sigmoid is meant for binary classification problems and so fits our dataset, while ReLu is known for modeling complex non-linear relationships, which is something we're trying to capture with the amount of features we're analyzing in this dataset.

Since our dataset has a relatively high number of features, we felt it was most appropriate to start with a layer with a higher number of neurons to allow the initial features to be captured. The second and third layers use a Geometric Pyramid to divide the number of neurons in half to prevent overfitting and to help the model learn more complex relationships.
*  We used **ReLU** activation functions for the first three layers to avoid the higher compute requirements of Tahn activation function.
*  We used **Sigmoid** activation functions for classification in our output layer to optimize for normalizing between 0 and 1.
*  We used **Adam** as our optimizer as a reliable default choice for faster convergence and robustness to noisy gradients.
*  We used **binary_crossentropy** as our loss function to optimize for a binary classification task of predicting hazardous and non-hazardous asteroids.

To fit the model, we chose an epoch of 40 and a batch size of 32, which seemed to provide the best balance between being able to quickly train the model and adjust the model while also not sacrificing its learning potential by using too few iterations or too small of a sample.

**Hyperparameter Tuning**

**Average Rule:** In testing our hyperparameters, we started by using the "Average Rule" to create two laters of small numbers neurons. However, this resulted in both low accuracy and greatly diminishing validation loss across epochs, suggesting we could increase the number of neurons to improve performance.

**Geometric Pyramid:** In adjusting hyperparameters, we identified that the large number of input features required more hidden layer nodes to handle feature complexity. Increasing the first layer to 64 neurons greatly improved training and validation accuracy before showing indications of overfitting.

**Batch Size:** Since our Y values are unevenly distributed, with hazardous asteroids undersampled at 16% of the dataset, we also increased the batch size to 32 to introduce a larger number of hazardous asteroids (5 of 32) into each epoch.

**Epochs:** After tuning hyperparameters, we observed that training and validation accuracy remained fairly consistent from 40 to 80 epochs. As such, we opted for the smaller number of epochs to enable faster training and limit overfitting.

# Hyperparameter Tuning
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

X = df_cleaned.drop('Hazardous_True', axis=1)
y = df_cleaned['Hazardous_True']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# This code builds a geometric pyramid neural network to handle the large numer of input features.
model = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# While experimenting with hyperparameters, we decided to save the epochs to value 'history' to visualize for overfitting.
history = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1)

# This code prints a model evaluation for test accuracy and loss after the final epoch.
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')

"""# Training and Validation Accuracy

To monitor for overfitting while tuning hyperparameters, we created the following charts to monitor where validation accuracy fell below training accuracy. In training, we observed best results at roughly 40 epochs using the parameters above.
"""

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

"""# Conclusion

Our team was successful in developing a neural network with reasonably high accuracy (97.5%) for testing data. This would allow astronomers to predict if an incoming asteroid was likely hazardous or not in the majority of cases.

We believe our model was aided by having a very clean dataset. Most of our features came ready-made as numeric data with no missing values given the nature of the problem set. As such, it seemed like the numeric nature of the data lent itself well to a neural network task, as even in the early epoch iterations our accuracy was relatively high compared to past assignments. It taught us the value of having good, clean data for the computer to mathmatecially model.

**Next Steps**

While useful in enabling researchers to identify potentially hazardous objects, this model could also help improve astronomers' observations by evaluating model weights to uncover which features are most correlated with hazaradous asteroids.

To improve this model, our team would further refine the model to maximize Recall and prioritize Type-1 errors. This would enable astronomers to detect the largest number of potentially hazaradous asteroids, even at the expense of higher false positive rates. Identifying hazardous asteroids is inherently a process of identifying low-probability, high-impact events.  Since the ramifications of failing to detect hazardous asteroids are potentially very high, and hazardous asteroids identified by the model are likely to be further scrutinized by astronomers, this would be a potentially significant improvement to our current model.

Another area for potential further investigation is that we were somewhat suspicious of overfitting. The model seemed to perform so well off the bat that we were concerned there might be something we missed causing it to overfit the training data, even if accuracy was generally consistent using the test data. One indicator of this phenomenon is that our model only performed equally to about the 28th training iteration on the test data. While that might seem fine in a vacuum, given that our model was only making marginal accuracy improvements in each iteration, it did represent a not insignificant difference in performance.
"""